## Natural Language Data Retriever

As organizations generate and store large volumes of data in diverse data stores, the ability to extract insights dynamically from that data or perform optimizations on it based on real-time trends becomes increasingly crucial. However, querying such diverse data stores requires writing complex database queries in languages such as SQL. This process is not only time-consuming and challenging for technically savvy users, but also poses significant barriers on non-technical users.  

Recent advances in generative AI have led to the rapid evolution of NL2SQL (Natural Language to SQL) technology, which leverages natural language as a querying mechanism to understand data. While this technology holds the promise of democratizing the ability to query complex data, converting natural language to complex database queries with accuracy and at scale using moderate compute is a significant challenge.
 
This code repository accompanies a blog post (__URL TBD__) that presents an approach that can simplify complex SQL query generation even when using simpler and cheaper models. The presented methodology can be adapted to other query languages. Our solution levels the field for all types of users by providing an approach that shifts the focus from writing queries to interpreting results, is cost-effective, and mitigates excessive reliance on technical experts. 

The solution achieves scalability through decoupling the natural language query from the specific queried data source. Given the natural language query, the appropriate data source is automatically determined by the solution. This abstraction allows for the solution to be data-source agnostic, data sources may be distributed across different clouds or be provided by different vendors, which will allow for it to support multiple data types and sources. The abstraction also allows for modularity within the solution and allows for the ability to quickly add new data sources as each new data source doesnâ€™t require its own solution. 

For an end-user, they only need to specify their query in natural language, and the solution determines which data source is the most suitable for answering that query, using contextual information present in the request to generate a query in the specific language that the data source requires (e.g., SQL).

This solution introduces a design pattern that decomposes the overall process of converting natural language (NL) to SQL into a series of smaller sub-problems. By breaking down the NL-to-SQL conversion into a sequence of focused steps, rather than a single large step, this approach enables the use of smaller Large Language Models (LLMs) for the task. Consequently, this method results in reduced computational requirements, decreased response latency, and improved accuracy.

## Files in the repository
This repository provides an example of how this solution can be implemented. The presentation is biased towards presenting the concepts in a simple to follow form, as opposed to developing a production grade solution. This should be used as a template to develop a more robust solution for your enterprise needs.

Following is the set of files and their purpose in the repository
### Core files
* `app_constants.py` - this file contains constants used across the implementation.
* `pre_process_request.py` - this files defines a class called pre-process-request. The primary function on this class is to pre-process a user request such that it is ready for down-stream processing. This is a simple implementation that is expected to be replaced with a more sophisticated AI in the full implementation.
* `identity_service_facade.py` - this file defines a class called IdentityServiceFacade. This class abstracts the interface to external identity service(s) which are able to identify "named resources" in input requests (such as a person's name), to a corresponding system identifier that can be used for SQL queries (or as an element of an API request for other use cases).

* `prepare_request.py` - This file defines a class called Prepare Request. The run method of this class takes a pre-processed request and return a dictionary that includes:
    * A LLM prompt to be used for CodeLlama SQL generations
    * A SQL script to be used along with the SQL generated by CodeLlama

* `llm_facade.py` - The class, LlmFacade, defined in this file, provides an abstraction to LLM services used in the implementation. For this example implementation this is wrapping the Amazon Bedrock service.

* `rdbms_facade.py` - The class RdbmsFacade wraps the SQL Database Engines(s) to provide a consistent interface independent of the underlying implementation

### Database files
`db_employee.db` and `db_olympics.db` are sqlite3 database files used for the illustrative example provided here.

### Execution flow
* `text_to_sql_flow.py` - This class, TextToSQLFlow, orchestrates the overall control-flow of the solution. The run method, given a user request in natural language will process the request, generate SQL corresponding to the request, run that SQL against the appropriate RBDMS and database and return the result.
* `text_to_sql_notebook.ipynb` - This is the same overall control-flow orchestration in a notebook form.

### Running tests
* `test_cases.py` - This file defines a class called TestCases.This class holds a set of example user requests that can be used to exercise the solution.
* `test_drive_text_to_sql_flow.py` - runs test cases from the `test_cases.py` file.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.

